{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>*****Part 1*****</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Go through each street in a specified zipcode and parse house number ranges from Melissa Data website\n",
    "## Then write to CSV (house number range, street, and zip)\n",
    "import csv\n",
    "\n",
    "# Import the library used to query the website\n",
    "import urllib.request\n",
    "# Import the Beautiful Soup functions to parse the data returned from the website\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "\n",
    "# Specify the URL\n",
    "data = 'https://www.melissadata.com/lookups/CartZip.asp?Zip=80203'\n",
    "\n",
    "# Query the website and return the html to the variable 'page'\n",
    "main_page = urllib.request.urlopen(data)\n",
    "\n",
    "# Parse the html in the 'main_page' variable and store it in the Beautiful Soup format\n",
    "soup = BeautifulSoup(main_page, \"html5lib\")\n",
    "\n",
    "list_of_carrier_routes = []\n",
    "base_url = 'https://www.melissadata.com/lookups/'\n",
    "\n",
    "with open('MelissaData_Scrape_80203_wholeScript_test.csv', 'w', newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        all_links = (link.get('href'))\n",
    "        print(all_links)\n",
    "        \n",
    "        if 'cartzip.asp' in all_links and all_links[-4][0] == 'C':\n",
    "            list_of_carrier_routes = (base_url + all_links)\n",
    "            #print(list_of_carrier_routes)\n",
    "            \n",
    "            route_text =[]\n",
    "            carrier_route_range = []\n",
    "            \n",
    "            page = urllib.request.urlopen(list_of_carrier_routes)\n",
    "\n",
    "            soup = BeautifulSoup(page, \"html5lib\")\n",
    "\n",
    "            for route in list_of_carrier_routes:\n",
    "                body = soup.find(\"body\")\n",
    "                \n",
    "            for x in body.find_all(\"font\"):\n",
    "                route_text = (x.text)\n",
    "\n",
    "                if route_text != \"Map\\xa0\":\n",
    "                    carrier_route_range.append(route_text)\n",
    "                    writer.writerow(carrier_route_range)\n",
    "                    carrier_route_range = []\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>*****Part 2*****</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Use a master list of CO zipcodes to iterate through each page containing all street numbers by zipcode\n",
    "\n",
    "import csv\n",
    "\n",
    "# Import the Beautiful Soup functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "with open(r'C:\\Users\\broadways\\CDOT_temp\\Employment_data\\CO_Master_Lists\\CO_Zipcodes_Cities.csv') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        zip_code = row[1]\n",
    "        data = \"http://www.melissadata.com/lookups/zipstreet.asp?InData=%s&c=1&l=U\" % zip_code\n",
    "        #print(data)\n",
    "# Query the website and return the html to the variable 'page'\n",
    "        streets_page = urllib.request.urlopen(data)\n",
    "# Parse the html in the 'streets_page' variable and store it in the Beautiful Soup format    \n",
    "        soup = BeautifulSoup(streets_page, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query the website and return the html to the variable 'page'\n",
    "streets_page = urllib.request.urlopen(data)\n",
    "\n",
    "# Import the Beautiful Soup functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parse the html in th e'page' variable and store it in the Beautiful Soup format\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n",
    "\n",
    "# Look at nested structure of HTML page\n",
    "#print(soup.prettify())\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    all_links = (link.get('href'))\n",
    "    print(all_links)\n",
    "    if all_links.startswith('zipstreet'):\n",
    "        zipcode = all_links.rsplit('Step5=',1)[0]\n",
    "        name = all_links.split('Name=')\n",
    "        print(zipcode, name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create lists from columns in CSV\n",
    "from csv import DictReader\n",
    "\n",
    "street_name=[]\n",
    "with open(r'C:\\Users\\broadways\\CDOT_temp\\HouseNumber_StreetAddress_80203.csv') as f:\n",
    "    house_number = [row[\"House Number\"] for row in DictReader(f)]\n",
    "    #print(house_number)\n",
    "with open(r'C:\\Users\\broadways\\CDOT_temp\\HouseNumber_StreetAddress_80203.csv') as f:\n",
    "        for row in DictReader(f):\n",
    "            if row[\"Unique Street Name\"] == '':\n",
    "                pass\n",
    "            else:\n",
    "                name = row[\"Unique Street Name\"]\n",
    "                street_name.append(name)\n",
    "                #print(street_name)\n",
    "#with open(r'C:\\Users\\broadways\\CDOT_temp\\HouseNumber_StreetAddress_80203_short.csv') as f:\n",
    "    #zip_code = [row[\"Zip Code\"] for row in DictReader(f)]\n",
    "    #print(zip_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Combining all possible combinations of House# and Street Name from Melissa Data\n",
    "\n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "combined = list(itertools.product(*[house_number, street_name]))\n",
    "#print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('combined_houseNumber_streetName_80203_all.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['House Number and Street Address'])\n",
    "    for x in combined:\n",
    "        writer.writerow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Simple Web Scrape</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simple use case using requests\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from tabulate import tabulate\n",
    " \n",
    "res = requests.get(\"https://cogcc.state.co.us/COGIS/DrillingPermitsList.cfm\")\n",
    "soup = BeautifulSoup(res.content,'lxml')\n",
    "table = soup.find_all('table')\n",
    "\n",
    "\n",
    "df = pd.read_html(str(table))\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geocoder_35",
   "language": "python",
   "name": "geocoder_35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
